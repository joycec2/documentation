{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc15fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ListIndex, SimpleDirectoryReader, VectorStoreIndex, Settings\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.ollama import  Ollama\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9e653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Settings.llm = Ollama(model=\"llama3.1:8b-instruct-fp16\", request_timeout=360.0, temperature=0.2)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d4d9e",
   "metadata": {},
   "source": [
    "# List Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b06855f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Rewrite**\n",
      "\n",
      "The Baltimore Ravens' offense is showing signs of improvement, particularly with Zay Flowers leading the charge. He had an impressive game against the Washington Commanders, setting multiple records for receiving yards in the first half and helping the team secure a win. Flowers has been heating up over the past two weeks, and his chemistry with Lamar Jackson and other receivers is evident. The Ravens' offense is confident in their abilities, and they're working hard to put it all together on a consistent basis. With players like Flowers making big plays, the team is optimistic about their chances of success moving forward.\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"./data/ravens_web_official_news_10_7_10_14\").load_data()\n",
    "index = ListIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What's new with Baltimore Ravens?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577966f",
   "metadata": {},
   "source": [
    "# Tree Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a063ca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Baltimore Ravens are riding a four-game winning streak, with their offense looking like a juggernaut. They have a balanced attack that is powered by running back Derrick Henry, who has been historically good this season. He has become the first player since Pro Football Hall of Famer LaDainian Tomlinson in 2005 to score a rushing touchdown in each of his first six games of a season. The Ravens are also just the second team in the Super Bowl era with at least 150 rushing yards and a rushing touchdown in each of its first six games of a season.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import TreeIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('./data/ravens_web_official_news_10_7_10_14').load_data()\n",
    "tree_index = TreeIndex.from_documents(documents)\n",
    "query_engine = tree_index.as_query_engine()\n",
    "response = query_engine.query(\"What's new with Baltimore Ravens?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120f2a2",
   "metadata": {},
   "source": [
    "# Keyword Table Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf527274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Rewrite**: The Ravens' defense has been trending upward after a physical performance against the Commanders, with several key players making bone-jarring hits and swarming to the football. This was a big game for Baltimore's defense after giving up five touchdown passes in Week 5, but they were able to raise their game by eliminating glaring mistakes and limiting Washington's longest play from scrimmage to just 28 yards. The addition of veteran coach Dean Pees has likely contributed to this improvement, as the team looks to build on its strong rush defense and shore up its secondary.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import KeywordTableIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('./data/ravens_web_official_news_10_7_10_14').load_data()\n",
    "keyword_index = KeywordTableIndex.from_documents(documents)\n",
    "query_engine = keyword_index.as_query_engine()\n",
    "response = query_engine.query(\"What's new with Baltimore Ravens?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd16a57",
   "metadata": {},
   "source": [
    "# Vector Store Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e20b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new collection 'ravens'.\n"
     ]
    }
   ],
   "source": [
    "# Create Chroma client\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "\n",
    "# Define collection name\n",
    "collection_name = \"ravens\"\n",
    "\n",
    "# Check if the collection already exists\n",
    "existing_collections = chroma_client.list_collections()\n",
    "\n",
    "if collection_name in [collection.name for collection in existing_collections]:\n",
    "    chroma_collection = chroma_client.get_collection(collection_name)\n",
    "    print(f\"Using existing collection '{collection_name}'.\")\n",
    "else:\n",
    "    chroma_collection = chroma_client.create_collection(collection_name)\n",
    "    print(f\"Created new collection '{collection_name}'.\")\n",
    "\n",
    "# Set up embedding model\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"snowflake-arctic-embed\",\n",
    "    ollama_additional_kwargs={\"prostatic\": 0},\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"./data/ravens_web_official_news_10_7_10_14\").load_data()\n",
    "\n",
    "# Set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context, embed_model=embed_model\n",
    "# )\n",
    "\n",
    "\n",
    "# with node\n",
    "from llama_index.core.node_parser import SentenceSplitter, SimpleNodeParser\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model, transformations=[SimpleNodeParser(chunk_size=512, chunk_overlap=20)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d3947",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ca303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Ravens' special teams unit has been performing well lately. They've had some clutch plays, including a game-tying field goal by Justin Tucker from 56 yards out against the Bengals. This marked a turning point in the game and showed that Tucker is still one of the most reliable kickers in the league.\n"
     ]
    }
   ],
   "source": [
    "# Create query engine and perform query\n",
    "query_engine = index.as_query_engine(similarity_top_k=1)\n",
    "response = query_engine.query(\"What's new with Baltimore Ravens special teams?\")\n",
    "# with open(\"Output.txt\", \"w\") as text_file:\n",
    "#     text_file.write(str(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adce7c6",
   "metadata": {},
   "source": [
    "# FLARE with ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d0027",
   "metadata": {},
   "source": [
    "## ChromaDB setup (copy from previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11d78321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new collection 'ravens'.\n"
     ]
    }
   ],
   "source": [
    "# Create Chroma client\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "\n",
    "# Define collection name\n",
    "collection_name = \"ravens\"\n",
    "\n",
    "# Check if the collection already exists\n",
    "existing_collections = chroma_client.list_collections()\n",
    "\n",
    "if collection_name in [collection.name for collection in existing_collections]:\n",
    "    chroma_collection = chroma_client.get_collection(collection_name)\n",
    "    print(f\"Using existing collection '{collection_name}'.\")\n",
    "else:\n",
    "    chroma_collection = chroma_client.create_collection(collection_name)\n",
    "    print(f\"Created new collection '{collection_name}'.\")\n",
    "\n",
    "# Set up embedding model\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"snowflake-arctic-embed\",\n",
    "    ollama_additional_kwargs={\"prostatic\": 0},\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"./data/ravens_web_official_news_10_7_10_14\").load_data()\n",
    "\n",
    "# Set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context, embed_model=embed_model\n",
    "# )\n",
    "\n",
    "\n",
    "# with node\n",
    "from llama_index.core.node_parser import SentenceSplitter, SimpleNodeParser\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model, transformations=[SimpleNodeParser(chunk_size=512, chunk_overlap=20)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e59e632",
   "metadata": {},
   "source": [
    "## FLARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query engine and perform query\n",
    "index_query_engine = index.as_query_engine(similarity_top_k=2)\n",
    "\n",
    "# create FLARE\n",
    "from llama_index.core.query_engine import FLAREInstructQueryEngine\n",
    "\n",
    "flare_query_engine = FLAREInstructQueryEngine(\n",
    "    query_engine=index_query_engine,\n",
    "    max_iterations=7,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8a578a",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb8318df",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index_query_engine.query(\"What's going on with Baltimore Ravens?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22c82b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The team is having a strong season, as evidenced by their recent division wins. They have also been performing well in game management, which is a key aspect of their success. Additionally, they are showing resilience and determination, as suggested by the comments from one of their players who feels judged despite limited playing time.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e5409d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;32mQuery: What's going on with Baltimore Ravens?\n",
      "\u001b[0m\u001b[1;3;34mCurrent response: \n",
      "\u001b[0m\u001b[1;3;38;5;200mLookahead response: [Search(What is happening with the Baltimore Ravens?)]\n",
      "\u001b[0m\u001b[1;3;38;5;200mUpdated lookahead response: Based on the provided information, I will fill in the lookahead templates with the appropriate answers.\n",
      "\n",
      "**Case 1:**\n",
      "\n",
      "Previous Response:\n",
      "\n",
      "Lookahead Template:\n",
      "Red is for \n",
      "\u001b[0m\u001b[1;3;34mCurrent response:  Based on the provided information, I will fill in the lookahead templates with the appropriate answers.\n",
      "\n",
      "**Case 1:**\n",
      "\n",
      "Previous Response:\n",
      "\n",
      "Lookahead Template:\n",
      "Red is for\n",
      "\u001b[0m\u001b[1;3;38;5;200mLookahead response: Based on the existing answer and the lookahead template, here's my attempt to generate the next portion of the answer:\n",
      "\n",
      "[Search(What color is associated with the Baltimore Ravens?)]\n",
      "\n",
      "\n",
      "I've used the Search API to look up relevant information about the Baltimore Ravens. If you'd like me to continue generating the answer or if it's complete, please let me know!\n",
      "\u001b[0m\u001b[1;3;38;5;200mUpdated lookahead response: Based on the previous response and lookahead template, I will fill in the answers as follows:\n",
      "\n",
      "Previous Response:\n",
      "One of the largest cities in the world\n",
      "\n",
      "Lookahead Template:\n",
      ", the city contains a population of \n",
      "\u001b[0m\u001b[1;3;34mCurrent response: Based on the provided information, I will fill in the lookahead templates with the appropriate answers.\n",
      "\n",
      "**Case 1:**\n",
      "\n",
      "Previous Response:\n",
      "\n",
      "Lookahead Template:\n",
      "Red is for Based on the previous response and lookahead template, I will fill in the answers as follows:\n",
      "\n",
      "Previous Response:\n",
      "One of the largest cities in the world\n",
      "\n",
      "Lookahead Template:\n",
      ", the city contains a population of\n",
      "\u001b[0m\u001b[1;3;38;5;200mLookahead response: Based on the existing answer and the lookahead template, I will fill in the answer as follows:\n",
      "\n",
      "Previous Response: One of the largest cities in the world\n",
      "Lookahead Template: , the city contains a population of\n",
      "Answer: [Search(What is the population of one of the largest cities in the world?)]\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m flare_response \u001b[38;5;241m=\u001b[39m \u001b[43mflare_query_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms going on with Baltimore Ravens?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:307\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    310\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/llama_index/core/base/base_query_engine.py:52\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     51\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 52\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m     54\u001b[0m     QueryEndEvent(query\u001b[38;5;241m=\u001b[39mstr_or_query_bundle, response\u001b[38;5;241m=\u001b[39mquery_result)\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:307\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    310\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/llama_index/core/query_engine/flare/base.py:238\u001b[0m, in \u001b[0;36mFLAREInstructQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    230\u001b[0m updated_lookahead_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lookahead_answer_inserter\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m    231\u001b[0m     lookahead_resp, query_tasks, query_answers, prev_response\u001b[38;5;241m=\u001b[39mcur_response\n\u001b[1;32m    232\u001b[0m )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# get \"relevant\" lookahead response by truncating the updated\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# lookahead response until the start position of the first tag\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# also remove the prefix from the lookahead response, so that\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# we can concatenate it with the existing response\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m relevant_lookahead_resp_wo_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_lookahead_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdated_lookahead_resp\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n\u001b[1;32m    243\u001b[0m     print_text(\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated lookahead response: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelevant_lookahead_resp_wo_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    246\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpink\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    247\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/llama_index/core/query_engine/flare/base.py:176\u001b[0m, in \u001b[0;36mFLAREInstructQueryEngine._get_relevant_lookahead_response\u001b[0;34m(self, updated_lookahead_resp)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get relevant lookahead response.\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# if there's remaining query tasks, then truncate the response\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# until the start position of the first tag\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# there may be remaining query tasks because the _max_lookahead_query_tasks\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# is less than the total number of generated [Search(query)] tags\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m remaining_query_tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query_task_output_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdated_lookahead_resp\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(remaining_query_tasks) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    180\u001b[0m     relevant_lookahead_resp \u001b[38;5;241m=\u001b[39m updated_lookahead_resp\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:307\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    310\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/llama_index/core/query_engine/flare/output_parser.py:60\u001b[0m, in \u001b[0;36mQueryTaskOutputParser.parse\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     58\u001b[0m         end_idx \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m     59\u001b[0m         raw_query_str \u001b[38;5;241m=\u001b[39m output[start_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m : end_idx]\n\u001b[0;32m---> 60\u001b[0m         query_str \u001b[38;5;241m=\u001b[39m \u001b[43mraw_query_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     61\u001b[0m         query_tasks\u001b[38;5;241m.\u001b[39mappend(QueryTask(query_str, start_idx, end_idx))\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_tasks\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "flare_response = flare_query_engine.query(\"What's going on with Baltimore Ravens?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cf08a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, I will fill in the lookahead templates with the appropriate answers.\n",
      "\n",
      "**First Example**\n",
      "\n",
      "Previous Response:\n",
      "\n",
      "Lookahead Template:\n",
      "Red is for Based on the provided information, I will fill in the lookahead templates with the appropriate answers.\n",
      "\n",
      "**First Example**\n",
      "\n",
      "Previous Response:\n",
      "\n",
      "Lookahead Template:\n",
      "Red is for Based on the previous response and the query-answer pairs, I will fill in the lookahead template as follows:\n",
      "\n",
      "Previous Response: The colors on the flag of Ghana have the following meanings. Red is for\n"
     ]
    }
   ],
   "source": [
    "print(flare_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87805b",
   "metadata": {},
   "source": [
    "# RAG with Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de22f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"./data/ravens_web_official_news_10_7_10_14\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9536e3",
   "metadata": {},
   "source": [
    "## Split docs into small chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf9c0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser.text import SentenceSplitter\n",
    "text_parser = SentenceSplitter(\n",
    " chunk_size=1024,\n",
    ")\n",
    "text_chunks = []\n",
    "doc_idxs = []\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    cur_text_chunks = text_parser.split_text(doc.text)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5181dc4",
   "metadata": {},
   "source": [
    "## Construct Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f3974b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "    text=text_chunk,\n",
    "    )\n",
    "    src_doc = documents[doc_idxs[idx]]\n",
    "    node.metadata = src_doc.metadata\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c5860",
   "metadata": {},
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a16cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up embedding model\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"snowflake-arctic-embed\",\n",
    "    ollama_additional_kwargs={\"prostatic\": 0},\n",
    ")\n",
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "    node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e3ff6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce82196",
   "metadata": {},
   "source": [
    "## Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "23ebd2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "client = qdrant_client.QdrantClient(location=\":memory:\")\n",
    "from llama_index.core import (\n",
    " VectorStoreIndex,\n",
    " StorageContext,\n",
    " SimpleDirectoryReader,\n",
    ")\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"my_collection\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58491bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    " documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "437f318c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['82f9b08d-4c2c-4cd6-aa2e-1a139bba5105',\n",
       " '301bb0ea-7de1-455a-9283-72bb666047b9',\n",
       " '5bfec3d9-a7a2-4fab-82a1-0e811ebcaf15',\n",
       " 'd2cab947-008a-4bee-9354-d91c65bc99fd',\n",
       " '529534f8-aa9f-46f9-9fd8-8b948c0b3090',\n",
       " 'b6eb8758-2114-4ac4-8e6d-89856ebdf763',\n",
       " '60463f07-5294-420a-88f5-431eaef54bdc',\n",
       " 'cc5b8f71-dc40-4920-80df-311a1e5383ec',\n",
       " 'ea99810b-4af8-420a-9b8f-721c4a87f612',\n",
       " 'd8ee931b-e5b8-4581-bf51-4458d7e2bd74',\n",
       " 'acc2ce09-265e-4b84-b7a7-52a8fbe2ec48',\n",
       " 'f99e2b81-10de-4136-9138-2830c907a6dc',\n",
       " 'fe003cdf-9d65-4627-9705-8ba9dd98b9aa',\n",
       " '9f6612ff-c101-4ac8-9abb-1e2d4a80ed0e',\n",
       " 'd68d2757-7cae-4d98-a343-9fbef6bcd9ea',\n",
       " '1e4518fb-d3f6-4039-85f6-42b84d9d0ea5',\n",
       " '0d67d06a-50d6-4a7a-921e-1bc57247560c',\n",
       " '3519df90-e624-44a5-9de5-df11baed480e',\n",
       " '86bafcba-38bd-4532-b329-ed103aebb6a6',\n",
       " '27b9ce36-0dd6-4727-b404-3c8e12e42b2f',\n",
       " '29d4da6d-c263-4c7f-8cec-6dbfcba98591',\n",
       " '427b5a35-e798-48b0-bc75-9f2911cee1e7',\n",
       " '8eee9fdc-9231-4374-9ab5-89334c5db82d',\n",
       " '45675450-355a-47d2-b6b2-dceb5729ce7b',\n",
       " 'eab98441-d745-4bf4-868f-2cb07ecf9408',\n",
       " 'c9225af3-ab9a-4375-bb1c-0564343c306e',\n",
       " 'e5bdbaaa-3b85-40a8-a364-7adb860a1b6d',\n",
       " 'f92649c3-894d-484d-92df-8b0c23354f95',\n",
       " '46b17ef2-65fe-40ab-8289-862cb20170f0',\n",
       " '3083edb5-5ad3-451b-b7ef-0837c0c607ee',\n",
       " '7c570ac0-4006-4b03-b3a7-b0cb3b21f19d',\n",
       " '829b1e06-b79b-4627-b8cd-82d92d1b500c',\n",
       " '45eaadc2-b868-45d4-9ae0-ee84e5eefa1d',\n",
       " 'e79ba64c-701e-469e-845f-1bbf19f8848c',\n",
       " '4868774b-d9da-45d8-96bf-6e007d48a68d',\n",
       " '9604bf14-3d2d-4aa4-acfa-489bcb3682d3',\n",
       " '3e2d9630-7ce5-4a95-97ba-a18c45d182aa',\n",
       " 'faf36dfc-f2dc-4718-ab7a-21d940217d86',\n",
       " '03f22cf5-8338-434b-a344-1bc6d559a53e',\n",
       " '06b8df43-fdd6-4621-b96a-9ba0550dcbd4',\n",
       " '3c86a63b-f410-4ae8-a891-3cd61773f2a7',\n",
       " 'ca457cb6-906b-47fe-a5b6-166693d579e0',\n",
       " 'e73e051d-8eb3-4e8f-a8ad-e183c8526144',\n",
       " '6483b870-7fa3-46e1-a699-869481f66993',\n",
       " 'c9e95bd0-200d-42ee-9596-f54462e942bd',\n",
       " '1107e5b2-5184-40a6-bc88-85dbfd3835a0',\n",
       " '2f3f4ae6-8574-4326-945c-95b1cf6b85ed',\n",
       " '1b6ffc91-24aa-4ee7-9d71-84f80c163936',\n",
       " '7028cc5e-610c-446c-b0ed-285b92555cc3',\n",
       " '6ce17607-eedd-41b0-b445-b3a322a30566',\n",
       " '6f18e6fb-4a49-4b97-af53-934933db2d3a',\n",
       " 'c796fa24-f721-404d-8b3e-cc418d1db4eb',\n",
       " '5440753a-4d51-4849-b4ca-93ee622caf96',\n",
       " '5d00accc-fe02-4bfb-97aa-419a3d1c45fd',\n",
       " '6a692cf6-1fb1-440c-9044-e1b6204332b3',\n",
       " 'afddbd6c-caae-495a-99b0-c74ce2da0055',\n",
       " 'a93afe68-364a-4a16-a742-5267b9a55ff0',\n",
       " '32430cdf-c95f-493a-9d1f-32b5796f52ac',\n",
       " '41ec9eb8-43e6-45f0-be5a-1a33263784ef',\n",
       " 'afdba69b-7798-4974-860a-7a7f08d9af8d',\n",
       " '896e4aee-47d8-41dd-9f98-36c3fa11a3bc',\n",
       " '9fded5a2-e963-48e2-a4b7-f1760eded5d1',\n",
       " 'da16fa7d-2e2f-46e6-97a3-0cec896b0991',\n",
       " '35749052-31dc-4932-bf3b-e945211dd3ec',\n",
       " '9009ddb8-d9bb-4a60-8c71-94c3851d0721',\n",
       " '6e3a1740-60e0-4f74-9d37-2a61cdd48299',\n",
       " 'dbbfa230-5477-4d15-84bf-f16ef430db97',\n",
       " '8643a0a2-a72f-4647-8642-d2724e3eaf18',\n",
       " 'd8216b7c-bd7c-4a95-8267-41a59b3d502e',\n",
       " '9cfe3146-0cb7-452a-9ea1-d79804dfccc7',\n",
       " '4c5a4141-0aff-491e-854f-0db649a955da',\n",
       " '9a01818b-a9e1-445b-a764-d02df5fb64f9',\n",
       " '9f6e2d09-d1c9-4ae7-8c11-cee7f3b14804',\n",
       " '9c2cf8e7-818e-41b6-b2ab-3d1937c19bd6',\n",
       " '38e2126b-d95d-43bf-a7b1-df95293a2118',\n",
       " 'a33ac245-4d4f-4505-959f-77ce3341f257',\n",
       " '39906d6c-502a-45d5-b901-875446da3965',\n",
       " 'aaa82986-9115-4073-8e17-186540bc5b7e',\n",
       " 'a4367142-c941-467a-a795-1b8613fd8d31',\n",
       " '06ccf077-4cc1-47e6-9e5b-e3a245f2a062',\n",
       " 'b11734f9-5fcc-4dd0-80df-1f5b6fa8cf1d',\n",
       " '49578cad-047b-44b9-9538-5e1375141be9',\n",
       " '6b3621d1-34f3-4fc0-b085-98998d27f1ee',\n",
       " '8ec8df97-6e9b-4630-835b-3a71326916d9']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d026c8",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "729f2172",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"what's new with Baltimore Ravens?\"\n",
    "query_embedding = embed_model.get_query_embedding(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdae278",
   "metadata": {},
   "source": [
    "## Construct Vector Store Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e60cafc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He converted big third down after big third down, did a terrific job re-discovering Mark Andrews, feeding Zay Flowers and finding guys like Tylan Wallace and Charlie Kolar when he needed them. The play to Isaiah Likely in the fourth quarter will run on highlight clips for perpetuity, and it should — but it shouldn't overshadow this gritty, brilliant performance. He looked like a two-time MVP.\"\n",
      "\n",
      "The Baltimore Banner's Jonas Shaffer: \"One of the first things Derrick Henry said after the game was that Lamar Jackson's the best player in the NFL. After all we saw today — the incredible highs and the very rare lows — who's going to tell him otherwise?\"\n",
      "\n",
      "The Baltimore Banner's Kyle Goon: \"A long time from now, when Jackson is enshrined in Canton, they'll play the highlight of him stiff-arming Sam Hubbard to the ground, then throwing a touchdown to Isaiah Likely across his body. The two-time MVP reminded us who he is with a masterpiece in the second half, including three touchdown passes and a lot of game-extending wizardry that only he is capable of conjuring.\"\n",
      "\n",
      "All Ravens Tight Ends Are Becoming a Matchup Problem\n",
      "Over the past two games, most plays made by the Ravens' tight end trio of Mark Andrews, Isaiah Likely, and Charlie Kolar were delivering blocks to free Henry for 350 yards and three touchdowns. But in the Bengals' concerted effort to slow down the Ravens' tailback, the tight end trio came alive in the pass game, and Pressbox’s Bo Smolka notes how they're challenging opposing defenses.\n",
      "\n",
      "\"The Ravens' tight ends continue to be a matchup problem for opposing defenses, and in multiple ways,\" Smolka wrote. \"Not only do they make catches, but they have been exceptional perimeter blockers this season, often wiping out smaller defensive backs to spring long runs, and they have become major decoys.\"\n",
      "\n",
      "Smolka noted their production wasn't the only thing impacting the Bengals' defense, but their presence, too.\n",
      "\n",
      "\"Asked this week about Andrews' low production this season, quarterback Lamar Jackson noted that when Andrews has been double-teamed, the field opens for others,\" Smolka wrote. \"Indeed, in the first quarter, the Ravens faced third-and-five at their 21-yard line. Many, many times over the past five years, Jackson has looked for Andrews in that exact situation. That's probably why the Bengals double-teamed Andrews. Instead, Jackson lofted a pass just beyond Andrews and those two defenders, to a wide-open Zay Flowers for a 26-yard gain. In the third quarter, Likely went in motion to the left, and after receiving the snap, Jackson looked that way. Then he quickly looked right and found Kolar wide-open for a 55-yard catch-and-run.\"\n",
      "\n",
      "'Ravens Were Right to Have Faith in Justin Tucker'\n",
      "The first-ever \"slump\" for Justin Tucker became a hefty talking point over the first quarter of the season. The rarity perplexed pundits and fans as the most accurate kicker in NFL history was struggling from distance while seemingly every other kicker was crushing kicks.\n",
      "\n",
      "But against the Bengals with fewer than two minutes to play and all the pressure on the kicking unit from 56 yards out, Tucker blasted the ball. Once it reached the apex – before splitting the uprights – three Ravens across the line signaled it was good. Tucker tied the game at 38.\n",
      "\n",
      "Russell Street Report’s Darin McCann praised Tucker for answering the call.\n",
      "\n",
      "\"I want to start by saying, that in one small way, the Ravens won this one because Justin Tucker delivered a clutch 50-plus yard field goal while the opponent's kicker missed his own opportunity,\" McCann wrote. \"Obviously, there was much more to this one than that, but let's give Tucker that today. It had to feel good for him to nail that kick.\"\n",
      "\n",
      "Zrebiec pointed out the Ravens have a few worries, but Tucker's name should not be on the list.\n",
      "\n",
      "\"The murmurs started a few weeks back when Tucker missed a few kicks at a time when NFL kickers were making field goals at an unbelievable rate. What was wrong with the most accurate kicker in the history of the NFL? As it turns out, nothing,\" Zrebiec wrote. \"… The 3-2 Ravens have some issues. They are too mistake prone. Their high-priced secondary is a concern. Tucker, however, isn't one of the team's issues.\"\n",
      "\n",
      "Defense Made Clutch Plays, But More is Expected\n",
      "Entering Sunday, the Ravens' secondary aimed to correct its pass defense ranking.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "query_mode = \"default\"\n",
    "# query_mode = \"sparse\"\n",
    "# query_mode = \"hybrid\"\n",
    "vector_store_query = VectorStoreQuery(\n",
    " query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    ")\n",
    "query_result = vector_store.query(vector_store_query)\n",
    "print(query_result.nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c0c7f",
   "metadata": {},
   "source": [
    "## Parse the Results into a set of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9070ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "nodes_with_scores = []\n",
    "for index, node in enumerate(query_result.nodes):\n",
    "    score: Optional[float] = None\n",
    "    if query_result.similarities is not None:\n",
    "        score = query_result.similarities[index]\n",
    "        nodes_with_scores.append(NodeWithScore(node=node, score=score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c521599f",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d9fadc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a qdrant vector store.\"\"\"\n",
    "    def __init__(\n",
    "    self,\n",
    "    vector_store: QdrantVectorStore,\n",
    "    embed_model: Any,\n",
    "    query_mode: str = \"default\",\n",
    "    similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "    \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        query_embedding = embed_model.get_query_embedding(\n",
    "        query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "        query_embedding=query_embedding,\n",
    "        similarity_top_k=self._similarity_top_k,\n",
    "        mode=self._query_mode,\n",
    "        )\n",
    "        query_result = vector_store.query(vector_store_query)\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "                nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "        return nodes_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1d35932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorDBRetriever(\n",
    "            vector_store, embed_model, query_mode=\"default\", similarity_top_k=2\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba63fc1",
   "metadata": {},
   "source": [
    "## Retriever Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63bf8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    " retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5abc4d",
   "metadata": {},
   "source": [
    "## Query Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "835948f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Baltimore Ravens have been on a roll lately. Their offense has been particularly impressive, with Lamar Jackson leading the charge. They've shown their ability to adapt and dominate games in different ways, whether it's through the air or on the ground. With players like Zay Flowers emerging as key contributors and Derrick Henry continuing to be a force to be reckoned with, this team is looking fierce.\n"
     ]
    }
   ],
   "source": [
    "query_str = \"what's new with Baltimore Ravens?\"\n",
    "response = query_engine.query(query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bd0057",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d54fbd73a6836902",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T07:11:36.617356Z",
     "start_time": "2024-08-07T07:10:31.936734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like several things are going well for the Ravens from what you've shared. They had multiple players contribute in the last game, their red-zone offense is doing great with 14 of their last 15 trips resulting in touchdowns, and Justin Tucker broke his skid with a big kick despite tough conditions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    system_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "response = chat_engine.chat(\"Is it good for Ravens?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"Is it good for Ravens?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
